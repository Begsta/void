{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speak now...\n",
      "Привет, как сам?\n",
      "You said:  Привет\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m r\u001b[39m.\u001b[39madjust_for_ambient_noise(source) \u001b[39m# to reduce noise\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m---> 11\u001b[0m     audio \u001b[39m=\u001b[39m r\u001b[39m.\u001b[39;49mlisten(source, phrase_time_limit\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m) \u001b[39m# record for up to 5 seconds\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     13\u001b[0m         text \u001b[39m=\u001b[39m r\u001b[39m.\u001b[39mrecognize_google(audio, language\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mru-RU\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m# use Google Speech Recognition API to transcribe the audio\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\speech_recognition\\__init__.py:491\u001b[0m, in \u001b[0;36mRecognizer.listen\u001b[1;34m(self, source, timeout, phrase_time_limit, snowboy_configuration)\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[39mif\u001b[39;00m timeout \u001b[39mand\u001b[39;00m elapsed_time \u001b[39m>\u001b[39m timeout:\n\u001b[0;32m    489\u001b[0m     \u001b[39mraise\u001b[39;00m WaitTimeoutError(\u001b[39m\"\u001b[39m\u001b[39mlistening timed out while waiting for phrase to start\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 491\u001b[0m buffer \u001b[39m=\u001b[39m source\u001b[39m.\u001b[39;49mstream\u001b[39m.\u001b[39;49mread(source\u001b[39m.\u001b[39;49mCHUNK)\n\u001b[0;32m    492\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(buffer) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m: \u001b[39mbreak\u001b[39;00m  \u001b[39m# reached end of the stream\u001b[39;00m\n\u001b[0;32m    493\u001b[0m frames\u001b[39m.\u001b[39mappend(buffer)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\speech_recognition\\__init__.py:199\u001b[0m, in \u001b[0;36mMicrophone.MicrophoneStream.read\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread\u001b[39m(\u001b[39mself\u001b[39m, size):\n\u001b[1;32m--> 199\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpyaudio_stream\u001b[39m.\u001b[39;49mread(size, exception_on_overflow\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pyaudio\\__init__.py:570\u001b[0m, in \u001b[0;36mPyAudio.Stream.read\u001b[1;34m(self, num_frames, exception_on_overflow)\u001b[0m\n\u001b[0;32m    567\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_input:\n\u001b[0;32m    568\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNot input stream\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    569\u001b[0m                   paCanNotReadFromAnOutputOnlyStream)\n\u001b[1;32m--> 570\u001b[0m \u001b[39mreturn\u001b[39;00m pa\u001b[39m.\u001b[39;49mread_stream(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stream, num_frames,\n\u001b[0;32m    571\u001b[0m                       exception_on_overflow)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "\n",
    "# initialize the recognizer\n",
    "r = sr.Recognizer()\n",
    "\n",
    "# open the microphone and start recording\n",
    "with sr.Microphone() as source:\n",
    "    print(\"Speak now...\")\n",
    "    r.adjust_for_ambient_noise(source) # to reduce noise\n",
    "    while True:\n",
    "        audio = r.listen(source) \n",
    "        try:\n",
    "            text = r.recognize_google(audio, language='ru-RU') # use Google Speech Recognition API to transcribe the audio\n",
    "            if text == 'привет' or text == 'Привет':\n",
    "                print('Привет, как сам?')\n",
    "            print(\"You said: \", text)\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Sorry, I couldn't understand what you said\")\n",
    "        except sr.RequestError as e:\n",
    "            print(\"Sorry, the speech recognition service is unavailable at the moment.\")\n",
    "        except KeyboardInterrupt:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speak now...\n",
      "You said:  Привет как твои дела\n",
      "Привет! У меня всё хорошо, спасибо. Я всего лишь программный ассистент, поэтому у меня нет личных дел. Как могу помочь вам?\n",
      "You said:  нейронная сеть которая сама себя озвучивает\n",
      "Извините, я не могу создать нейронную сеть, которая сама себя озвучивает, так как я не имею доступа к аудио-интерфейсу и не могу производить звуковые сигналы. Однако, я могу создать голосового помощника, который будет озвучивать текстовые ответы на вопросы пользователей.\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "import openai\n",
    "import gtts\n",
    "from playsound import playsound\n",
    "import os\n",
    "\n",
    "\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "# initialize the recognizerp\n",
    "r = sr.Recognizer()\n",
    "\n",
    "# open the microphone and start recording\n",
    "with sr.Microphone() as source:\n",
    "    print(\"Speak now...\")\n",
    "    r.adjust_for_ambient_noise(source) # to reduce noise\n",
    "    while True:\n",
    "        audio = r.listen(source) # record for up to 5 seconds\n",
    "        try:\n",
    "            text = r.recognize_google(audio, language='ru-RU') # use Google Speech Recognition API to transcribe the audio\n",
    "            print(\"You said: \", text)\n",
    "            # use OpenAI API to analyze the text and generate a response\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=\"gpt-3.5-turbo-0301\",\n",
    "                messages=[{'role':'system', 'content': text}],\n",
    "                max_tokens=1024,\n",
    "                n=1,\n",
    "                stop=None,\n",
    "                temperature=0.5\n",
    "                )\n",
    "            text = response.choices[0].message.content\n",
    "            print(text)\n",
    "            #print(\"AI response:\", response.choices[0].message.content)\n",
    "            tts = gtts.gTTS(str(text), lang='ru')\n",
    "            tts.save(\"hello.mp3\")\n",
    "            playsound(\"hello.mp3\")\n",
    "            os.remove('hello.mp3')\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Sorry, I couldn't understand what you said\")\n",
    "        except sr.RequestError as e:\n",
    "            print(\"Sorry, the speech recognition service is unavailable at the moment.\")\n",
    "        except KeyboardInterrupt:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gtts\n",
    "from playsound import playsound\n",
    "tts = gtts.gTTS(\"Привет\", lang='ru')\n",
    "tts.save(\"hello.mp3\")\n",
    "playsound(\"hello.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio</th>\n",
       "      <th>numpy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pud_ability_devour_14_ru</td>\n",
       "      <td>[[-80.0, -80.0, -80.0, -80.0, -80.0, -78.48184...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pud_ability_devour_15_ru</td>\n",
       "      <td>[[-80.0, -80.0, -80.0, -73.111725, -64.158936,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pud_ability_devour_16_ru</td>\n",
       "      <td>[[-80.0, -80.0, -80.0, -80.0, -80.0, -69.63963...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pud_ability_heap_01_ru</td>\n",
       "      <td>[[-80.0, -80.0, -80.0, -80.0, -74.60346, -52.4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pud_ability_heap_02_ru</td>\n",
       "      <td>[[-80.0, -80.0, -80.0, -80.0, -79.32175, -70.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>Ха-ха! Свежее мясо!</td>\n",
       "      <td>[[-80.0, -80.0, -80.0, -80.0, -80.0, -72.69466...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>Хе-хе-хе-хе-хе-хе-хе, свежее мясо!</td>\n",
       "      <td>[[-80.0, -80.0, -80.0, -80.0, -80.0, -57.26712...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>Хе-хе-хе-хе. Свежее мясо.</td>\n",
       "      <td>[[-80.0, -80.0, -80.0, -76.28006, -51.98091, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>Хе-хе-хе-хе. Свежее мясо</td>\n",
       "      <td>[[-80.0, -80.0, -80.0, -80.0, -80.0, -71.41070...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>Хы-хы-хы-хы, свежее мясо!</td>\n",
       "      <td>[[-80.0, -80.0, -80.0, -80.0, -80.0, -70.64865...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  audio   \n",
       "0              Pud_ability_devour_14_ru  \\\n",
       "1              Pud_ability_devour_15_ru   \n",
       "2              Pud_ability_devour_16_ru   \n",
       "3                Pud_ability_heap_01_ru   \n",
       "4                Pud_ability_heap_02_ru   \n",
       "..                                  ...   \n",
       "195                 Ха-ха! Свежее мясо!   \n",
       "196  Хе-хе-хе-хе-хе-хе-хе, свежее мясо!   \n",
       "197           Хе-хе-хе-хе. Свежее мясо.   \n",
       "198            Хе-хе-хе-хе. Свежее мясо   \n",
       "199           Хы-хы-хы-хы, свежее мясо!   \n",
       "\n",
       "                                                 numpy  \n",
       "0    [[-80.0, -80.0, -80.0, -80.0, -80.0, -78.48184...  \n",
       "1    [[-80.0, -80.0, -80.0, -73.111725, -64.158936,...  \n",
       "2    [[-80.0, -80.0, -80.0, -80.0, -80.0, -69.63963...  \n",
       "3    [[-80.0, -80.0, -80.0, -80.0, -74.60346, -52.4...  \n",
       "4    [[-80.0, -80.0, -80.0, -80.0, -79.32175, -70.0...  \n",
       "..                                                 ...  \n",
       "195  [[-80.0, -80.0, -80.0, -80.0, -80.0, -72.69466...  \n",
       "196  [[-80.0, -80.0, -80.0, -80.0, -80.0, -57.26712...  \n",
       "197  [[-80.0, -80.0, -80.0, -76.28006, -51.98091, -...  \n",
       "198  [[-80.0, -80.0, -80.0, -80.0, -80.0, -71.41070...  \n",
       "199  [[-80.0, -80.0, -80.0, -80.0, -80.0, -70.64865...  \n",
       "\n",
       "[200 rows x 2 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.DataFrame()\n",
    "\n",
    "path = './audio/'\n",
    "path_for_data = './numpy_data/'\n",
    "\n",
    "\n",
    "\n",
    "for name in os.listdir(path):\n",
    "    new_row = {'audio':name[:-4], 'numpy': np.load(f'./numpy_data/{name[:-3]+\"npy\"}')}\n",
    "    dataframe = pd.concat([dataframe, pd.DataFrame([new_row])], ignore_index=True)\n",
    "dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "audio    Pud_ability_devour_14_ru.wav\n",
       "numpy                            wnpy\n",
       "dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series({'audio':name, 'numpy': str(name[-3] + 'npy')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Texts: [[1, 3, 38, 39, 2], [1, 3, 38, 40, 2], [1, 3, 38, 41, 2], [1, 3, 49, 4, 2], [1, 3, 49, 5, 2], [1, 3, 6, 4, 2], [1, 3, 6, 5, 2], [1, 3, 6, 7, 2], [1, 3, 6, 8, 2], [1, 3, 6, 12, 2], [1, 3, 6, 13, 2], [1, 3, 6, 14, 2], [1, 3, 6, 18, 2], [1, 3, 6, 19, 2], [1, 3, 6, 23, 2], [1, 3, 6, 24, 4, 2], [1, 3, 6, 24, 5, 2], [1, 3, 6, 24, 7, 2], [1, 3, 6, 24, 8, 2], [1, 3, 6, 24, 12, 2], [1, 3, 6, 24, 13, 2], [1, 3, 6, 24, 18, 2], [1, 3, 6, 24, 19, 2], [1, 3, 6, 24, 23, 2], [1, 3, 27, 5, 2], [1, 3, 27, 14, 2], [1, 3, 27, 18, 2], [1, 3, 27, 19, 2], [1, 3, 27, 23, 2], [1, 3, 27, 28, 2], [1, 3, 27, 35, 2], [1, 3, 27, 42, 2], [1, 25, 4, 2], [1, 25, 5, 2], [1, 25, 7, 2], [1, 25, 8, 2], [1, 25, 12, 2], [1, 25, 13, 2], [1, 25, 14, 2], [1, 25, 18, 2], [1, 25, 19, 2], [1, 32, 4, 2], [1, 32, 5, 2], [1, 32, 7, 2], [1, 32, 8, 2], [1, 32, 12, 2], [1, 9, 4, 2], [1, 9, 5, 2], [1, 9, 7, 2], [1, 9, 8, 2], [1, 9, 12, 2], [1, 9, 13, 2], [1, 9, 18, 2], [1, 9, 19, 2], [1, 9, 23, 2], [1, 9, 28, 2], [1, 9, 35, 2], [1, 9, 42, 2], [1, 9, 39, 2], [1, 9, 40, 2], [1, 9, 41, 2], [1, 58, 4, 2], [1, 50, 4, 2], [1, 50, 5, 2], [1, 43, 4, 2], [1, 43, 5, 2], [1, 43, 7, 2], [1, 36, 4, 2], [1, 36, 5, 2], [1, 36, 7, 2], [1, 36, 8, 2], [1, 51, 4, 2], [1, 51, 5, 2], [1, 20, 4, 2], [1, 20, 5, 2], [1, 20, 7, 2], [1, 20, 8, 2], [1, 20, 12, 2], [1, 20, 13, 2], [1, 20, 14, 2], [1, 20, 18, 2], [1, 20, 19, 2], [1, 20, 23, 2], [1, 20, 28, 2], [1, 15, 4, 2], [1, 15, 5, 2], [1, 15, 7, 2], [1, 15, 8, 2], [1, 15, 12, 2], [1, 15, 13, 2], [1, 15, 14, 2], [1, 15, 18, 2], [1, 15, 19, 2], [1, 15, 23, 2], [1, 15, 28, 2], [1, 15, 35, 2], [1, 52, 4, 2], [1, 52, 5, 2], [1, 59, 4, 2], [1, 53, 4, 2], [1, 53, 5, 2], [1, 33, 4, 2], [1, 33, 5, 2], [1, 33, 7, 2], [1, 33, 8, 2], [1, 33, 12, 2], [1, 54, 4, 2], [1, 54, 5, 2], [1, 55, 4, 2], [1, 55, 5, 2], [1, 44, 4, 2], [1, 44, 5, 2], [1, 44, 7, 2], [1, 56, 4, 2], [1, 56, 5, 2], [1, 57, 4, 2], [1, 57, 5, 2], [1, 29, 4, 2], [1, 29, 5, 2], [1, 29, 7, 2], [1, 29, 8, 2], [1, 29, 12, 2], [1, 29, 13, 2], [1, 29, 14, 2], [1, 21, 4, 2], [1, 21, 5, 2], [1, 21, 7, 2], [1, 21, 8, 2], [1, 21, 12, 2], [1, 21, 13, 2], [1, 21, 14, 2], [1, 21, 18, 2], [1, 21, 19, 2], [1, 21, 23, 2], [1, 21, 28, 2], [1, 45, 46, 4, 2], [1, 45, 46, 5, 2], [1, 45, 46, 7, 2], [1, 26, 4, 2], [1, 26, 5, 2], [1, 26, 7, 2], [1, 26, 8, 2], [1, 26, 12, 2], [1, 26, 13, 2], [1, 26, 14, 2], [1, 26, 18, 2], [1, 26, 19, 2], [1, 30, 4, 2], [1, 30, 5, 2], [1, 30, 7, 2], [1, 30, 8, 2], [1, 30, 12, 2], [1, 30, 13, 2], [1, 30, 14, 2], [1, 22, 4, 2], [1, 22, 5, 2], [1, 22, 7, 2], [1, 22, 8, 2], [1, 22, 12, 2], [1, 22, 13, 2], [1, 22, 14, 2], [1, 22, 18, 2], [1, 22, 19, 2], [1, 22, 23, 2], [1, 22, 28, 2], [1, 31, 4, 2], [1, 31, 5, 2], [1, 31, 7, 2], [1, 31, 8, 2], [1, 31, 13, 2], [1, 31, 14, 2], [1, 31, 18, 2], [1, 10, 4, 2], [1, 10, 7, 2], [1, 10, 8, 2], [1, 10, 12, 2], [1, 10, 13, 2], [1, 10, 14, 2], [1, 10, 19, 2], [1, 10, 23, 2], [1, 10, 28, 2], [1, 10, 35, 2], [1, 10, 42, 2], [1, 10, 39, 2], [1, 10, 40, 2], [1, 10, 41, 2], [1, 10, 60, 2], [34, 34, 34, 16, 17], [47, 47, 47, 16, 17], [61, 16, 17], [16, 17], [16, 17], [16, 17], [62], [48, 34, 34, 16, 17], [48, 48, 16, 17], [11, 11, 11, 11, 11, 11, 11, 16, 17], [11, 11, 11, 11, 16, 17], [11, 11, 11, 11, 16, 17], [37, 37, 37, 37, 16, 17]]\n",
      "Encoded Texts:\n",
      " [[1, 3, 38, 39, 2], [1, 3, 38, 40, 2], [1, 3, 38, 41, 2], [1, 3, 49, 4, 2], [1, 3, 49, 5, 2], [1, 3, 6, 4, 2], [1, 3, 6, 5, 2], [1, 3, 6, 7, 2], [1, 3, 6, 8, 2], [1, 3, 6, 12, 2], [1, 3, 6, 13, 2], [1, 3, 6, 14, 2], [1, 3, 6, 18, 2], [1, 3, 6, 19, 2], [1, 3, 6, 23, 2], [1, 3, 6, 24, 4, 2], [1, 3, 6, 24, 5, 2], [1, 3, 6, 24, 7, 2], [1, 3, 6, 24, 8, 2], [1, 3, 6, 24, 12, 2], [1, 3, 6, 24, 13, 2], [1, 3, 6, 24, 18, 2], [1, 3, 6, 24, 19, 2], [1, 3, 6, 24, 23, 2], [1, 3, 27, 5, 2], [1, 3, 27, 14, 2], [1, 3, 27, 18, 2], [1, 3, 27, 19, 2], [1, 3, 27, 23, 2], [1, 3, 27, 28, 2], [1, 3, 27, 35, 2], [1, 3, 27, 42, 2], [1, 25, 4, 2], [1, 25, 5, 2], [1, 25, 7, 2], [1, 25, 8, 2], [1, 25, 12, 2], [1, 25, 13, 2], [1, 25, 14, 2], [1, 25, 18, 2], [1, 25, 19, 2], [1, 32, 4, 2], [1, 32, 5, 2], [1, 32, 7, 2], [1, 32, 8, 2], [1, 32, 12, 2], [1, 9, 4, 2], [1, 9, 5, 2], [1, 9, 7, 2], [1, 9, 8, 2], [1, 9, 12, 2], [1, 9, 13, 2], [1, 9, 18, 2], [1, 9, 19, 2], [1, 9, 23, 2], [1, 9, 28, 2], [1, 9, 35, 2], [1, 9, 42, 2], [1, 9, 39, 2], [1, 9, 40, 2], [1, 9, 41, 2], [1, 58, 4, 2], [1, 50, 4, 2], [1, 50, 5, 2], [1, 43, 4, 2], [1, 43, 5, 2], [1, 43, 7, 2], [1, 36, 4, 2], [1, 36, 5, 2], [1, 36, 7, 2], [1, 36, 8, 2], [1, 51, 4, 2], [1, 51, 5, 2], [1, 20, 4, 2], [1, 20, 5, 2], [1, 20, 7, 2], [1, 20, 8, 2], [1, 20, 12, 2], [1, 20, 13, 2], [1, 20, 14, 2], [1, 20, 18, 2], [1, 20, 19, 2], [1, 20, 23, 2], [1, 20, 28, 2], [1, 15, 4, 2], [1, 15, 5, 2], [1, 15, 7, 2], [1, 15, 8, 2], [1, 15, 12, 2], [1, 15, 13, 2], [1, 15, 14, 2], [1, 15, 18, 2], [1, 15, 19, 2], [1, 15, 23, 2], [1, 15, 28, 2], [1, 15, 35, 2], [1, 52, 4, 2], [1, 52, 5, 2], [1, 59, 4, 2], [1, 53, 4, 2], [1, 53, 5, 2], [1, 33, 4, 2], [1, 33, 5, 2], [1, 33, 7, 2], [1, 33, 8, 2], [1, 33, 12, 2], [1, 54, 4, 2], [1, 54, 5, 2], [1, 55, 4, 2], [1, 55, 5, 2], [1, 44, 4, 2], [1, 44, 5, 2], [1, 44, 7, 2], [1, 56, 4, 2], [1, 56, 5, 2], [1, 57, 4, 2], [1, 57, 5, 2], [1, 29, 4, 2], [1, 29, 5, 2], [1, 29, 7, 2], [1, 29, 8, 2], [1, 29, 12, 2], [1, 29, 13, 2], [1, 29, 14, 2], [1, 21, 4, 2], [1, 21, 5, 2], [1, 21, 7, 2], [1, 21, 8, 2], [1, 21, 12, 2], [1, 21, 13, 2], [1, 21, 14, 2], [1, 21, 18, 2], [1, 21, 19, 2], [1, 21, 23, 2], [1, 21, 28, 2], [1, 45, 46, 4, 2], [1, 45, 46, 5, 2], [1, 45, 46, 7, 2], [1, 26, 4, 2], [1, 26, 5, 2], [1, 26, 7, 2], [1, 26, 8, 2], [1, 26, 12, 2], [1, 26, 13, 2], [1, 26, 14, 2], [1, 26, 18, 2], [1, 26, 19, 2], [1, 30, 4, 2], [1, 30, 5, 2], [1, 30, 7, 2], [1, 30, 8, 2], [1, 30, 12, 2], [1, 30, 13, 2], [1, 30, 14, 2], [1, 22, 4, 2], [1, 22, 5, 2], [1, 22, 7, 2], [1, 22, 8, 2], [1, 22, 12, 2], [1, 22, 13, 2], [1, 22, 14, 2], [1, 22, 18, 2], [1, 22, 19, 2], [1, 22, 23, 2], [1, 22, 28, 2], [1, 31, 4, 2], [1, 31, 5, 2], [1, 31, 7, 2], [1, 31, 8, 2], [1, 31, 13, 2], [1, 31, 14, 2], [1, 31, 18, 2], [1, 10, 4, 2], [1, 10, 7, 2], [1, 10, 8, 2], [1, 10, 12, 2], [1, 10, 13, 2], [1, 10, 14, 2], [1, 10, 19, 2], [1, 10, 23, 2], [1, 10, 28, 2], [1, 10, 35, 2], [1, 10, 42, 2], [1, 10, 39, 2], [1, 10, 40, 2], [1, 10, 41, 2], [1, 10, 60, 2], [34, 34, 34, 16, 17], [47, 47, 47, 16, 17], [61, 16, 17], [16, 17], [16, 17], [16, 17], [62], [48, 34, 34, 16, 17], [48, 48, 16, 17], [11, 11, 11, 11, 11, 11, 11, 16, 17], [11, 11, 11, 11, 16, 17], [11, 11, 11, 11, 16, 17], [37, 37, 37, 37, 16, 17]]\n",
      "Padded Texts:\n",
      " [[ 1  3 38 ...  0  0  0]\n",
      " [ 1  3 38 ...  0  0  0]\n",
      " [ 1  3 38 ...  0  0  0]\n",
      " ...\n",
      " [11 11 11 ...  0  0  0]\n",
      " [11 11 11 ...  0  0  0]\n",
      " [37 37 37 ...  0  0  0]]\n",
      "Vocabulary Size: 63\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Sample text data\n",
    "texts = dataframe['audio']\n",
    "\n",
    "# Text normalization and tokenization\n",
    "normalized_texts = []\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "for text in texts:\n",
    "    # Normalize text by converting to lowercase and removing punctuation\n",
    "    normalized_text = text.lower()\n",
    "    # Tokenize the normalized text\n",
    "    tokens = tokenizer.texts_to_sequences([normalized_text])[0]\n",
    "    normalized_texts.append(tokens)\n",
    "\n",
    "# Text encoding\n",
    "encoded_texts = normalized_texts\n",
    "\n",
    "# Padding\n",
    "max_seq_length = max(len(seq) for seq in encoded_texts)\n",
    "padded_texts = pad_sequences(encoded_texts, maxlen=max_seq_length, padding='post')\n",
    "\n",
    "# Convert to numpy array\n",
    "padded_texts = np.array(padded_texts)\n",
    "\n",
    "# Vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Print the preprocessed data\n",
    "print(\"Normalized Texts:\", normalized_texts)\n",
    "print(\"Encoded Texts:\\n\", encoded_texts)\n",
    "print(\"Padded Texts:\\n\", padded_texts)\n",
    "print(\"Vocabulary Size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.12.0-cp311-cp311-win_amd64.whl (1.9 kB)\n",
      "Collecting tensorflow-intel==2.12.0\n",
      "  Using cached tensorflow_intel-2.12.0-cp311-cp311-win_amd64.whl (272.9 MB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\matebook\\documents\\pyt\\.conda\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (23.5.9)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting h5py>=2.9.0\n",
      "  Using cached h5py-3.8.0-cp311-cp311-win_amd64.whl (2.6 MB)\n",
      "Collecting jax>=0.3.15\n",
      "  Using cached jax-0.4.10-py3-none-any.whl\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\matebook\\documents\\pyt\\.conda\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (16.0.0)\n",
      "Requirement already satisfied: numpy<1.24,>=1.22 in c:\\users\\matebook\\documents\\pyt\\.conda\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.23.5)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\matebook\\documents\\pyt\\.conda\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\matebook\\documents\\pyt\\.conda\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (4.23.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\matebook\\documents\\pyt\\.conda\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (66.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\matebook\\documents\\pyt\\.conda\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\matebook\\documents\\pyt\\.conda\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\matebook\\documents\\pyt\\.conda\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (4.5.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\matebook\\documents\\pyt\\.conda\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\matebook\\documents\\pyt\\.conda\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (1.54.0)\n",
      "Collecting tensorboard<2.13,>=2.12\n",
      "  Using cached tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in c:\\users\\matebook\\documents\\pyt\\.conda\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.12.0)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in c:\\users\\matebook\\documents\\pyt\\.conda\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (2.12.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\matebook\\documents\\pyt\\.conda\\lib\\site-packages (from tensorflow-intel==2.12.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\matebook\\documents\\pyt\\.conda\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.12.0->tensorflow) (0.38.4)\n",
      "Collecting ml-dtypes>=0.1.0\n",
      "  Using cached ml_dtypes-0.1.0-cp311-cp311-win_amd64.whl (120 kB)\n",
      "Requirement already satisfied: scipy>=1.7 in c:\\users\\matebook\\documents\\pyt\\.conda\\lib\\site-packages (from jax>=0.3.15->tensorflow-intel==2.12.0->tensorflow) (1.10.1)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Using cached google_auth-2.18.1-py2.py3-none-any.whl (178 kB)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5\n",
      "  Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.4.3-py3-none-any.whl (93 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\matebook\\documents\\pyt\\.conda\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2.30.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\matebook\\documents\\pyt\\.conda\\lib\\site-packages (from tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.7.0)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Using cached Werkzeug-2.3.4-py3-none-any.whl (242 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Using cached cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "Requirement already satisfied: urllib3<2.0 in c:\\users\\matebook\\documents\\pyt\\.conda\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (1.26.15)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\matebook\\documents\\pyt\\.conda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\matebook\\documents\\pyt\\.conda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\matebook\\documents\\pyt\\.conda\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (2023.5.7)\n",
      "Collecting MarkupSafe>=2.1.1\n",
      "  Using cached MarkupSafe-2.1.2-cp311-cp311-win_amd64.whl (16 kB)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\matebook\\documents\\pyt\\.conda\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\matebook\\documents\\pyt\\.conda\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-intel==2.12.0->tensorflow) (3.2.2)\n",
      "Installing collected packages: rsa, pyasn1-modules, opt-einsum, ml-dtypes, MarkupSafe, markdown, h5py, google-pasta, gast, cachetools, astunparse, absl-py, werkzeug, requests-oauthlib, jax, google-auth, google-auth-oauthlib, tensorboard, tensorflow-intel, tensorflow\n",
      "Successfully installed MarkupSafe-2.1.2 absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.0 gast-0.4.0 google-auth-2.18.1 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 h5py-3.8.0 jax-0.4.10 markdown-3.4.3 ml-dtypes-0.1.0 opt-einsum-3.3.0 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.12.3 tensorflow-2.12.0 tensorflow-intel-2.12.0 werkzeug-2.3.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'text_data.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlosses\u001b[39m.\u001b[39mmean_squared_error(y_true, y_pred)\n\u001b[0;32m     38\u001b[0m \u001b[39m# Load training data\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m text \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39mtext_data.npy\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     40\u001b[0m mel_spec \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mmel_data.npy\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     42\u001b[0m \u001b[39m# Create and compile model\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\MateBook\\Documents\\pyt\\.conda\\Lib\\site-packages\\numpy\\lib\\npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    403\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 405\u001b[0m     fid \u001b[39m=\u001b[39m stack\u001b[39m.\u001b[39menter_context(\u001b[39mopen\u001b[39m(os_fspath(file), \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m    406\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    408\u001b[0m \u001b[39m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'text_data.npy'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define model architecture\n",
    "class Tacotron(tf.keras.Model):\n",
    "    def init(self):\n",
    "        super().init()\n",
    "        # Encoder network\n",
    "        self.encoder = tf.keras.layers.LSTM(256, return_sequences=True)\n",
    "        # Attention mechanism\n",
    "        self.attention = tf.keras.layers.Attention()\n",
    "        # Decoder network\n",
    "        self.decoder = tf.keras.layers.LSTM(256, return_sequences=True)\n",
    "        # Post-processing layer\n",
    "        self.postnet = tf.keras.layers.Conv1D(80, kernel_size=5, padding='same')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Split inputs into text and mel-spectrogram\n",
    "        text, mel_spec = inputs\n",
    "        # Encode text\n",
    "        encoded_text = self.encoder(text)\n",
    "        # Compute attention weights\n",
    "        attention_weights = self.attention([encoded_text, mel_spec])\n",
    "        # Apply attention\n",
    "        attention_output = tf.matmul(attention_weights, encoded_text, transpose_a=True)\n",
    "        # Concatenate attention output and mel-spectrogram\n",
    "        decoder_input = tf.concat([attention_output, mel_spec], axis=-1)\n",
    "        # Decode spectrogram\n",
    "        decoder_output = self.decoder(decoder_input)\n",
    "        # Apply post-processing\n",
    "        postnet_output = self.postnet(decoder_output)\n",
    "        # Return mel-spectrogram prediction\n",
    "        return postnet_output\n",
    "\n",
    "# Define loss function\n",
    "def mel_spectrogram_loss(y_true, y_pred):\n",
    "    return tf.keras.losses.mean_squared_error(y_true, y_pred)\n",
    "\n",
    "# Load training data\n",
    "text = np.load('text_data.npy')\n",
    "mel_spec = np.load('mel_data.npy')\n",
    "\n",
    "# Create and compile model\n",
    "model = Tacotron()\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=1e-3), loss=mel_spectrogram_loss)\n",
    "\n",
    "# Train model\n",
    "model.fit([, mel_spec], mel_spec, batch_size=16, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Texts: [[66, 67], [68, 69], [70, 20, 71], [72, 20, 73], [74, 20, 75], [35, 76], [35, 25, 77], [78, 79], [80, 20, 81], [82], [6, 83], [6, 26, 21, 84], [6, 85, 27, 36], [6, 6, 6, 3, 4], [6, 6, 6], [37, 86], [37], [28, 14, 87], [38], [88, 89, 7, 90, 91, 92], [93, 94, 15, 39, 95], [13, 96], [13, 97, 98, 99], [100, 101], [26, 21, 102, 103, 104], [29, 30, 7, 105, 31], [29, 30, 7, 106, 31], [29, 30, 7, 107, 31], [40, 108], [41, 13, 109, 110], [111, 112, 113, 114, 115, 116], [117], [118, 119, 120], [42, 43], [44, 45, 8, 27], [44, 121, 46, 122], [123, 124], [47, 13, 48], [47, 13, 48], [125, 126, 127], [128, 129, 8, 130], [131], [132, 133, 134], [49, 22, 16, 16, 16], [135, 136, 137], [50, 138, 139, 51, 140], [52, 141, 5, 142, 143, 144, 145], [52, 146, 147, 148], [2, 2, 26, 17, 8, 149, 150, 151], [2, 2, 28, 2, 2], [2, 2, 28, 2], [2, 2, 2, 3, 4], [2, 2, 2, 152, 153, 154], [2, 2, 2, 2], [2, 2], [155, 156], [157, 158, 159], [51], [160, 11, 11], [12, 161, 162, 17, 49, 22], [12, 163, 164], [12, 53], [5, 165, 54, 166], [5, 167, 168, 41, 21, 169], [5, 170], [5, 171, 18, 39], [5, 172], [5, 6], [5, 16, 55], [5, 16, 55], [173], [174, 3, 4], [175, 56, 23, 23], [19], [19, 19, 19], [19, 17, 8, 24, 176], [45, 177], [178, 57], [179, 180, 181], [58, 59], [58, 59], [182, 183, 32, 15, 60, 13, 184, 5, 185], [186, 187, 5, 188], [189, 25, 7, 190, 191], [192, 193], [194], [195], [196, 15, 12, 53, 8, 197, 198, 199, 200, 18, 54, 61], [201, 202, 22], [203, 12, 204], [205, 7, 206], [23, 23, 207, 208], [209, 210], [211, 7, 212, 213], [214, 13, 215, 17, 216, 15], [61, 40], [3, 4], [3, 4], [3, 4], [3, 4], [3, 4], [217, 218], [219], [62, 7, 220], [62], [221, 222, 12, 223, 6, 8, 224], [225, 226, 227], [36, 27], [33, 228, 229, 230], [33, 46, 21, 231], [232, 233], [63, 33, 63], [234, 22], [14, 60, 5, 235], [14, 236, 237, 238], [239, 11], [32, 15, 240, 56, 241], [242], [10, 6, 6, 3, 4], [10, 6], [10, 10, 3, 4], [10, 10, 38], [10], [1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 3, 4], [1, 1, 1, 1, 1, 1, 1, 3, 4], [1, 1, 1, 1, 3, 4], [1, 1, 1, 1, 3, 4], [1, 1, 1, 1], [243, 2, 244, 245, 12, 246], [247, 57], [9, 9, 10, 10, 14, 248, 3, 4], [9, 9, 9, 9, 3, 4], [9, 9, 9, 9, 32, 25, 249, 250], [251, 1, 1, 1, 1, 1, 5, 252], [253, 42, 43], [64, 7, 65], [64, 7, 65], [254, 255], [24, 256], [24, 18, 8, 257], [11, 11, 11], [11, 11], [34], [34, 14], [34], [17, 50, 258, 12, 259], [260], [8, 261, 262, 5, 263], [8, 264, 265], [8, 24, 18, 5, 18, 266]]\n",
      "Encoded Texts:\n",
      " [[66, 67], [68, 69], [70, 20, 71], [72, 20, 73], [74, 20, 75], [35, 76], [35, 25, 77], [78, 79], [80, 20, 81], [82], [6, 83], [6, 26, 21, 84], [6, 85, 27, 36], [6, 6, 6, 3, 4], [6, 6, 6], [37, 86], [37], [28, 14, 87], [38], [88, 89, 7, 90, 91, 92], [93, 94, 15, 39, 95], [13, 96], [13, 97, 98, 99], [100, 101], [26, 21, 102, 103, 104], [29, 30, 7, 105, 31], [29, 30, 7, 106, 31], [29, 30, 7, 107, 31], [40, 108], [41, 13, 109, 110], [111, 112, 113, 114, 115, 116], [117], [118, 119, 120], [42, 43], [44, 45, 8, 27], [44, 121, 46, 122], [123, 124], [47, 13, 48], [47, 13, 48], [125, 126, 127], [128, 129, 8, 130], [131], [132, 133, 134], [49, 22, 16, 16, 16], [135, 136, 137], [50, 138, 139, 51, 140], [52, 141, 5, 142, 143, 144, 145], [52, 146, 147, 148], [2, 2, 26, 17, 8, 149, 150, 151], [2, 2, 28, 2, 2], [2, 2, 28, 2], [2, 2, 2, 3, 4], [2, 2, 2, 152, 153, 154], [2, 2, 2, 2], [2, 2], [155, 156], [157, 158, 159], [51], [160, 11, 11], [12, 161, 162, 17, 49, 22], [12, 163, 164], [12, 53], [5, 165, 54, 166], [5, 167, 168, 41, 21, 169], [5, 170], [5, 171, 18, 39], [5, 172], [5, 6], [5, 16, 55], [5, 16, 55], [173], [174, 3, 4], [175, 56, 23, 23], [19], [19, 19, 19], [19, 17, 8, 24, 176], [45, 177], [178, 57], [179, 180, 181], [58, 59], [58, 59], [182, 183, 32, 15, 60, 13, 184, 5, 185], [186, 187, 5, 188], [189, 25, 7, 190, 191], [192, 193], [194], [195], [196, 15, 12, 53, 8, 197, 198, 199, 200, 18, 54, 61], [201, 202, 22], [203, 12, 204], [205, 7, 206], [23, 23, 207, 208], [209, 210], [211, 7, 212, 213], [214, 13, 215, 17, 216, 15], [61, 40], [3, 4], [3, 4], [3, 4], [3, 4], [3, 4], [217, 218], [219], [62, 7, 220], [62], [221, 222, 12, 223, 6, 8, 224], [225, 226, 227], [36, 27], [33, 228, 229, 230], [33, 46, 21, 231], [232, 233], [63, 33, 63], [234, 22], [14, 60, 5, 235], [14, 236, 237, 238], [239, 11], [32, 15, 240, 56, 241], [242], [10, 6, 6, 3, 4], [10, 6], [10, 10, 3, 4], [10, 10, 38], [10], [1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 3, 4], [1, 1, 1, 1, 1, 1, 1, 3, 4], [1, 1, 1, 1, 3, 4], [1, 1, 1, 1, 3, 4], [1, 1, 1, 1], [243, 2, 244, 245, 12, 246], [247, 57], [9, 9, 10, 10, 14, 248, 3, 4], [9, 9, 9, 9, 3, 4], [9, 9, 9, 9, 32, 25, 249, 250], [251, 1, 1, 1, 1, 1, 5, 252], [253, 42, 43], [64, 7, 65], [64, 7, 65], [254, 255], [24, 256], [24, 18, 8, 257], [11, 11, 11], [11, 11], [34], [34, 14], [34], [17, 50, 258, 12, 259], [260], [8, 261, 262, 5, 263], [8, 264, 265], [8, 24, 18, 5, 18, 266]]\n",
      "Padded Texts:\n",
      " [[ 66  67   0 ...   0   0   0]\n",
      " [ 68  69   0 ...   0   0   0]\n",
      " [ 70  20  71 ...   0   0   0]\n",
      " ...\n",
      " [  8 261 262 ...   0   0   0]\n",
      " [  8 264 265 ...   0   0   0]\n",
      " [  8  24  18 ...   0   0   0]]\n",
      "Vocabulary Size: 267\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Sample text data\n",
    "texts = ['Blink Dagger!', 'Force Staff!', 'Heart of Tarrasque!', 'Hood of Defiance!', 'Pipe of Insight!', 'Pudge заждался !', 'Pudge тебя приголубит!', \"Shiva's Guard!\", 'Urn of Shadows!', 'Vanguard!', 'А Агаю', 'А вот и нет.', 'А ну быстро сюда!', 'А-а-а, свежее мясо!', 'А-а-а', 'Ага, иду.', 'Ага', 'Ах ты мелкий...', 'Бессмертие!', 'Будешь красоткой с яблоком во рту!', 'Будто одного меня было мало.', 'В кучу!', 'В самом расцвете сил.', 'Вдохни поглубже!', 'Вот и за мной пришли.', 'Враг ушел с верхней линии!', 'Враг ушел с нижней линии!', 'Враг ушел с центральной линии!', 'Время нарезать!', 'Всё в кишках, красота!', 'Вы поглядите, сколько сочного мяса вокруг!', 'Выпотрошить!', 'Давай к делу!', 'Давно пора!', 'Дай освежую... я быстро!', 'Дай подправлю тебе фигуру!', 'Двойной урон!', 'Дело в шляпе!!', 'Дело в шляпе!', 'Дышите глубже, ребята!', 'Жадина-говядина, я такой.', 'Иллюзия!', 'Какой симпатичный фарш!', 'Ко мне-е-е-е!', 'Кому свежих рёбрышек.', 'Кто ребро потерял. Моё будет.', 'Люблю внезапность не приходится чистить кишки вручную.', 'Люблю сокращать чужие калории.', 'М-м, вот это я понимаю - свежий воздух!', 'М-м-ах-м-м', 'М-м-ах-м', 'М-м-м, свежее мясо!', 'М-м-м, устрой лёгким праздник!', 'М-м-м-м', 'М-м', 'Мешки плоти.', 'Милый запах, чё', 'Моё!', 'Мэ-э-э.', 'На верную смерть - это ко мне!', 'На кого замахиваешься!', 'На кусочки!', 'Не пропадать же добру.', 'Не стану тратить всё и сразу.', 'Не твое!', 'Не тут-то было!', 'Не устоял!', 'Не-а!', 'Не-е-ет!', 'Не-е-ет.', 'Невидимость!', 'О, свежее мясо!', 'од топор-р-р!', 'Ой!', 'Ой-ой-ой.', 'Ой... это я что ли', 'Освежую, шмакодявка.', 'Отличная заточка.', 'Отложу, авось пригодится.', 'Паршивый волшебник!!', 'Паршивый волшебник!', 'Первая кровь! У меня ничего в зубах не застряло', 'Перед смертью не надышишься.', 'Подам тебя с криповым соусом!', 'Познакомимся поближе.', 'Поймал!', 'Получай!', 'Порежьте меня на кусочки, я бы сделал для вас то же самое.', 'Последний кусок - мне.', 'Припасу на потом.', 'Пшёл с дороги!', 'Р-р-расчехляю ножи!', 'Резать, рубить!', 'Рублю с двойным усердием!', 'Руки в крови - это про меня.', 'Самое время!', 'Свежее мясо!!!', 'Свежее мясо!!', 'Свежее мясо!', 'Свежее мясо.', 'Свежее мясо', 'Скоростная мясорубка!', 'Слезь!', 'Сожру с потрохами!', 'сожру', 'Солнце уходит на запад, а я остаюсь!', 'Спасёт от передряги.', 'Сюда, быстро!', 'Так пир или... чума!', 'Так тебе и надо!', 'Тип того', 'Туда так туда', 'Тушку мне!', 'Ты ничего не почувствуешь!', 'Ты сегодня без ужина!', 'Тэ-э.', 'У меня уже топор чешется!', 'Ускорение!', 'Ха-а-а! Свежее мясо!', 'Ха-а', 'Ха-ха! Свежее мясо!', 'Ха-ха, бессмертие!', 'Ха', 'Хе-хе хе', 'Хе-хе-хе-хе-хе-хе-хе, свежее мясо!!', 'Хе-хе-хе-хе-хе-хе-хе, свежее мясо!', 'Хе-хе-хе-хе. Свежее мясо.', 'Хе-хе-хе-хе. Свежее мясо', 'Хе-хе-хе-хе', 'Хм-м, попробуй жизнь на вкус!', 'Хорошая заточка.', 'Хы-хы-ха-ха, ты погляди, свежее мясо!', 'Хы-хы-хы-хы, свежее мясо!', 'Хы-хы-хы-хы, у тебя рёбра отвалились!', 'Хэх, хе-хе-хе-хе-хе, не твоё!', 'Чёрт, давно пора!', 'Чёрта... с два.', 'Чёрта... с два', 'Чёртовы крипы.', 'Что протухло', 'Что-то я... проголодался.', 'Э-э-э.', 'Э-э.', 'Эй!', 'Эй, ты.', 'Эй', 'Это кто зашел на ужин', 'Эх.', 'Я ж ещё... не дорезал.', 'Я специально, честно...', 'Я что-то не то съел']\n",
    "\n",
    "# Text normalization and tokenization\n",
    "normalized_texts = []\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "for text in texts:\n",
    "    # Normalize text by converting to lowercase and removing punctuation\n",
    "    normalized_text = text.lower()\n",
    "    # Tokenize the normalized text\n",
    "    tokens = tokenizer.texts_to_sequences([normalized_text])[0]\n",
    "    normalized_texts.append(tokens)\n",
    "\n",
    "# Text encoding\n",
    "encoded_texts = normalized_texts\n",
    "\n",
    "# Padding\n",
    "max_seq_length = max(len(seq) for seq in encoded_texts)\n",
    "padded_texts = pad_sequences(encoded_texts, maxlen=max_seq_length, padding='post')\n",
    "\n",
    "# Convert to numpy array\n",
    "padded_texts = np.array(padded_texts)\n",
    "\n",
    "# Vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Print the preprocessed data\n",
    "print(\"Normalized Texts:\", normalized_texts)\n",
    "print(\"Encoded Texts:\\n\", encoded_texts)\n",
    "print(\"Padded Texts:\\n\", padded_texts)\n",
    "print(\"Vocabulary Size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = padded_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 12, 80\n  y sizes: 80\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 50\u001b[0m\n\u001b[0;32m     47\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mAdam(lr\u001b[39m=\u001b[39m\u001b[39m1e-3\u001b[39m), loss\u001b[39m=\u001b[39mmel_spectrogram_loss)\n\u001b[0;32m     49\u001b[0m \u001b[39m# Train model\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m model\u001b[39m.\u001b[39;49mfit([text, mel_spec], mel_spec, batch_size\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m, epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\MateBook\\Documents\\pyt\\.conda\\Lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\MateBook\\Documents\\pyt\\.conda\\Lib\\site-packages\\keras\\engine\\data_adapter.py:1852\u001b[0m, in \u001b[0;36m_check_data_cardinality\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   1845\u001b[0m     msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m  \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m sizes: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1846\u001b[0m         label,\n\u001b[0;32m   1847\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\n\u001b[0;32m   1848\u001b[0m             \u001b[39mstr\u001b[39m(i\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(single_data)\n\u001b[0;32m   1849\u001b[0m         ),\n\u001b[0;32m   1850\u001b[0m     )\n\u001b[0;32m   1851\u001b[0m msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mMake sure all arrays contain the same number of samples.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1852\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n",
      "\u001b[1;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 12, 80\n  y sizes: 80\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Define model architecture\n",
    "class Tacotron(tf.keras.Model):\n",
    "    def init(self):\n",
    "        super().init()\n",
    "        # Encoder network\n",
    "        self.encoder = tf.keras.layers.LSTM(256, return_sequences=True)\n",
    "        # Attention mechanism\n",
    "        self.attention = tf.keras.layers.Attention()\n",
    "        # Decoder network\n",
    "        self.decoder = tf.keras.layers.LSTM(256, return_sequences=True)\n",
    "        # Post-processing layer\n",
    "        self.postnet = tf.keras.layers.Conv1D(80, kernel_size=5, padding='same')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Split inputs into text and mel-spectrogram\n",
    "        text, mel_spec = inputs\n",
    "        # Encode text\n",
    "        encoded_text = self.encoder(text)\n",
    "        # Compute attention weights\n",
    "        attention_weights = self.attention([encoded_text, mel_spec])\n",
    "        # Apply attention\n",
    "        attention_output = tf.matmul(attention_weights, encoded_text, transpose_a=True)\n",
    "        # Concatenate attention output and mel-spectrogram\n",
    "        decoder_input = tf.concat([attention_output, mel_spec], axis=-1)\n",
    "        # Decode spectrogram\n",
    "        decoder_output = self.decoder(decoder_input)\n",
    "        # Apply post-processing\n",
    "        postnet_output = self.postnet(decoder_output)\n",
    "        # Return mel-spectrogram prediction\n",
    "        return postnet_output\n",
    "\n",
    "# Define loss function\n",
    "def mel_spectrogram_loss(y_true, y_pred):\n",
    "    return tf.keras.losses.mean_squared_error(y_true, y_pred)\n",
    "\n",
    "# Load training data\n",
    "text = padded_texts[0]\n",
    "array_speck = []\n",
    "\n",
    "mel_spec = np.load('./numpy_data/Blink Dagger!.npy')\n",
    "\n",
    "# Create and compile model\n",
    "model = Tacotron()\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=1e-3), loss=mel_spectrogram_loss)\n",
    "\n",
    "# Train model\n",
    "model.fit([text, mel_spec], mel_spec, batch_size=16, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
