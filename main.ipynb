{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from scipy.io import wavfile\n",
    "import librosa\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Reshape\n",
    "from tensorflow.keras.metrics import *\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загружали дата фрейм с аудио и преобразованием аудио"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "audio    сожру.wav\n",
       "numpy         wnpy\n",
       "dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.DataFrame()\n",
    "\n",
    "path = './audio/'\n",
    "path_for_data = './numpy_data/'\n",
    "\n",
    "\n",
    "\n",
    "for name in sorted(os.listdir(path)):\n",
    "    new_row = {'audio':name[:-4], 'numpy': np.load(f'./numpy_data/{name[:-3]+\"npy\"}')}\n",
    "    dataframe = pd.concat([dataframe, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "pd.Series({'audio':name, 'numpy': str(name[-3] + 'npy')})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Преобразовывали предложения в цифры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Texts:\n",
      " [[6, 66], [6, 25, 20, 67], [6, 68, 26, 35], [6, 6, 6, 3, 4], [6, 6, 6], [36, 69], [36], [27, 14, 70], [37], [71, 72], [73, 74, 7, 75, 76, 77], [78, 79, 15, 38, 80], [13, 81], [13, 82, 83, 84], [85], [86, 87], [25, 20, 88, 89, 90], [28, 29, 7, 91, 30], [28, 29, 7, 92, 30], [28, 29, 7, 93, 30], [39, 94], [40, 13, 95, 96], [97, 98, 99, 100, 101, 102], [103], [104, 105, 106], [41, 42], [43, 44, 8, 26], [43, 107, 45, 108], [109, 110], [46, 13, 47], [46, 13, 47], [111, 112, 113], [114, 115, 8, 116], [117], [118, 119, 120], [48, 21, 16, 16, 16], [121, 122, 123], [49, 124, 125, 50, 126], [51, 127, 5, 128, 129, 130, 131], [51, 132, 133, 134], [2, 2, 25, 17, 8, 135, 136, 137], [2, 2, 27, 2, 2], [2, 2, 27, 2], [2, 2, 2, 3, 4], [2, 2, 2, 138, 139, 140], [2, 2, 2, 2], [2, 2], [141, 142], [143, 144, 145], [50], [146, 11, 11], [12, 147, 148, 17, 48, 21], [12, 149, 150], [12, 52], [5, 151, 53, 152], [5, 153, 154, 40, 20, 155], [5, 156], [5, 157, 18, 38], [5, 158], [5, 6], [5, 16, 54], [5, 16, 54], [159], [160, 3, 4], [161, 55, 22, 22], [19], [19, 19, 19], [19, 17, 8, 23, 162], [44, 163], [164, 56], [165, 166, 167], [57, 168], [57, 31, 169], [170, 24, 171], [58, 59], [58, 59], [172, 173, 32, 15, 60, 13, 174, 5, 175], [176, 177, 5, 178], [179, 31, 7, 180, 181], [182, 183], [184], [185], [186, 15, 12, 52, 8, 187, 188, 189, 190, 18, 53, 61], [191, 192, 21], [193, 12, 194], [195, 7, 196], [22, 22, 197, 198], [199, 200], [201, 7, 202, 203], [204, 13, 205, 17, 206, 15], [61, 39], [3, 4], [3, 4], [3, 4], [3, 4], [3, 4], [207, 208], [209], [62, 7, 210], [62], [211, 212, 12, 213, 6, 8, 214], [215, 216, 217], [35, 26], [33, 218, 219, 220], [33, 45, 20, 221], [222, 223], [63, 33, 63], [224, 21], [14, 60, 5, 225], [14, 226, 227, 228], [229, 11], [32, 15, 230, 55, 231], [232, 24, 233], [234], [235, 236], [10, 6, 6, 3, 4], [10, 6], [10, 10, 3, 4], [10, 10, 37], [10], [237, 24, 238], [1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 3, 4], [1, 1, 1, 1, 1, 1, 1, 3, 4], [1, 1, 1, 1, 3, 4], [1, 1, 1, 1, 3, 4], [1, 1, 1, 1], [239, 2, 240, 241, 12, 242], [243, 56], [244, 24, 245], [9, 9, 10, 10, 14, 246, 3, 4], [9, 9, 9, 9, 3, 4], [9, 9, 9, 9, 32, 31, 247, 248], [249, 1, 1, 1, 1, 1, 5, 250], [251, 41, 42], [64, 7, 65], [64, 7, 65], [252, 253], [23, 254], [23, 18, 8, 255], [256, 257], [11, 11, 11], [11, 11], [34], [34, 14], [34], [17, 49, 258, 12, 259], [260], [8, 261, 262, 5, 263], [8, 264, 265], [8, 23, 18, 5, 18, 266]]\n",
      "Padded Texts:\n",
      " [[  6  66   0 ...   0   0   0]\n",
      " [  6  25  20 ...   0   0   0]\n",
      " [  6  68  26 ...   0   0   0]\n",
      " ...\n",
      " [  8 261 262 ...   0   0   0]\n",
      " [  8 264 265 ...   0   0   0]\n",
      " [  8  23  18 ...   0   0   0]]\n",
      "Vocabulary Size: 267\n"
     ]
    }
   ],
   "source": [
    "# Sample text data\n",
    "texts = ['А Агаю', 'А вот и нет.', 'А ну быстро сюда!', 'А-а-а, свежее мясо!', 'А-а-а', 'Ага, иду.', 'Ага', 'Ах ты мелкий...', 'Бессмертие!', 'Блинк дагер!', 'Будешь красоткой с яблоком во рту!', 'Будто одного меня было мало.', 'В кучу!', 'В самом расцвете сил.', 'Вангвард!', 'Вдохни поглубже!', 'Вот и за мной пришли.', 'Враг ушел с верхней линии!', 'Враг ушел с нижней линии!', 'Враг ушел с центральной линии!', 'Время нарезать!', 'Всё в кишках, красота!', 'Вы поглядите, сколько сочного мяса вокруг!', 'Выпотрошить!', 'Давай к делу!', 'Давно пора!', 'Дай освежую... я быстро!', 'Дай подправлю тебе фигуру!', 'Двойной урон!', 'Дело в шляпе!!', 'Дело в шляпе!', 'Дышите глубже, ребята!', 'Жадина-говядина, я такой.', 'Иллюзия!', 'Какой симпатичный фарш!', 'Ко мне-е-е-е!', 'Кому свежих рёбрышек.', 'Кто ребро потерял. Моё будет.', 'Люблю внезапность не приходится чистить кишки вручную.', 'Люблю сокращать чужие калории.', 'М-м, вот это я понимаю - свежий воздух!', 'М-м-ах-м-м', 'М-м-ах-м', 'М-м-м, свежее мясо!', 'М-м-м, устрой лёгким праздник!', 'М-м-м-м', 'М-м', 'Мешки плоти.', 'Милый запах, чё', 'Моё!', 'Мэ-э-э.', 'На верную смерть - это ко мне!', 'На кого замахиваешься!', 'На кусочки!', 'Не пропадать же добру.', 'Не стану тратить всё и сразу.', 'Не твое!', 'Не тут-то было!', 'Не устоял!', 'Не-а!', 'Не-е-ет!', 'Не-е-ет.', 'Невидимость!', 'О, свежее мясо!', 'од топор-р-р!', 'Ой!', 'Ой-ой-ой.', 'Ой... это я что ли', 'Освежую, шмакодявка.', 'Отличная заточка.', 'Отложу, авось пригодится.', 'Падж заждался !', 'Падж тебя приголубит!', 'Пайп оф Инсайт!', 'Паршивый волшебник!!', 'Паршивый волшебник!', 'Первая кровь! У меня ничего в зубах не застряло', 'Перед смертью не надышишься.', 'Подам тебя с криповым соусом!', 'Познакомимся поближе.', 'Поймал!', 'Получай!', 'Порежьте меня на кусочки, я бы сделал для вас то же самое.', 'Последний кусок - мне.', 'Припасу на потом.', 'Пшёл с дороги!', 'Р-р-расчехляю ножи!', 'Резать, рубить!', 'Рублю с двойным усердием!', 'Руки в крови - это про меня.', 'Самое время!', 'Свежее мясо!!!', 'Свежее мясо!!', 'Свежее мясо!', 'Свежее мясо.', 'Свежее мясо', 'Скоростная мясорубка!', 'Слезь!', 'Сожру с потрохами!', 'сожру', 'Солнце уходит на запад, а я остаюсь!', 'Спасёт от передряги.', 'Сюда, быстро!', 'Так пир или... чума!', 'Так тебе и надо!', 'Тип того', 'Туда так туда', 'Тушку мне!', 'Ты ничего не почувствуешь!', 'Ты сегодня без ужина!', 'Тэ-э.', 'У меня уже топор чешется!', 'Урн оф Шадоувс!', 'Ускорение!', 'Форс стафф!', 'Ха-а-а! Свежее мясо!', 'Ха-а', 'Ха-ха! Свежее мясо!', 'Ха-ха, бессмертие!', 'Ха', 'Харт оф тараск!', 'Хе-хе хе', 'Хе-хе-хе-хе-хе-хе-хе, свежее мясо!!', 'Хе-хе-хе-хе-хе-хе-хе, свежее мясо!', 'Хе-хе-хе-хе. Свежее мясо.', 'Хе-хе-хе-хе. Свежее мясо', 'Хе-хе-хе-хе', 'Хм-м, попробуй жизнь на вкус!', 'Хорошая заточка.', 'Худ оф Дефайнс!', 'Хы-хы-ха-ха, ты погляди, свежее мясо!', 'Хы-хы-хы-хы, свежее мясо!', 'Хы-хы-хы-хы, у тебя рёбра отвалились!', 'Хэх, хе-хе-хе-хе-хе, не твоё!', 'Чёрт, давно пора!', 'Чёрта... с два.', 'Чёрта... с два', 'Чёртовы крипы.', 'Что протухло', 'Что-то я... проголодался.', 'Шивас Гвардс!', 'Э-э-э.', 'Э-э.', 'Эй!', 'Эй, ты.', 'Эй', 'Это кто зашел на ужин', 'Эх.', 'Я ж ещё... не дорезал.', 'Я специально, честно...', 'Я что-то не то съел']\n",
    "\n",
    "# Text normalization and tokenization\n",
    "\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "\n",
    "def texts_to_padded(texts):\n",
    "    normalized_texts = []\n",
    "    global tokenizer\n",
    "    for text in texts:\n",
    "        # Normalize text by converting to lowercase and removing punctuation\n",
    "        normalized_text = text.lower()\n",
    "        # Tokenize the normalized text\n",
    "        tokens = tokenizer.texts_to_sequences([normalized_text])[0]\n",
    "        normalized_texts.append(tokens)\n",
    "\n",
    "    # Text encoding\n",
    "    encoded_texts = normalized_texts\n",
    "\n",
    "    # Padding\n",
    "    max_seq_length = max(len(seq) for seq in encoded_texts)\n",
    "    padded_texts = pad_sequences(encoded_texts, maxlen=12, padding='post')\n",
    "\n",
    "    # Convert to numpy array\n",
    "    padded_texts = np.array(padded_texts)\n",
    "    return padded_texts, encoded_texts\n",
    "\n",
    "padded_texts, encoded_texts = texts_to_padded(texts)\n",
    "\n",
    "# Vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Print the preprocessed data\n",
    "#print(\"Normalized Texts:\", normalized_texts)\n",
    "print(\"Encoded Texts:\\n\", encoded_texts)\n",
    "print(\"Padded Texts:\\n\", padded_texts)\n",
    "print(\"Vocabulary Size:\", vocab_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загружаем спектрограмму "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrograms = [np.load(f'./numpy_data/{file_name}') for file_name in sorted(os.listdir('./numpy_data'))]\n",
    "\n",
    "ayst = max([i.shape[1] for i in spectrograms])\n",
    "def norm_spectr(spectrograms):\n",
    "    a = np.zeros((80,ayst)) -80\n",
    "    b = spectrograms\n",
    "    a[:, :b.shape[1]] = b\n",
    "    return a\n",
    "spectrograms = [norm_spectr(i) for i in spectrograms]\n",
    "spectrograms=np.array(spectrograms)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Спектрограмму преобразовываем в звук"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_for_data = './numpy_data/'\n",
    "\n",
    "def spectrogram_to_audio(  mel_spec_db: np.array\n",
    "                         , output_path: str = './audio2/'\n",
    "                         , output_filename: str ='output.wav') -> None:\n",
    "\n",
    "    mel_spec = librosa.db_to_power(mel_spec_db)\n",
    "\n",
    "    # Reconstruct audio signal from mel-spectrogram\n",
    "    audio = librosa.feature.inverse.mel_to_audio(mel_spec, sr=22050, n_fft=1024, hop_length=256)\n",
    "\n",
    "    # Rescale audio to the range [-1, 1]\n",
    "    audio /= np.max(np.abs(audio))\n",
    " \n",
    "    output_path = os.path.join(output_path, output_filename)\n",
    "    wavfile.write(output_path, 22050, audio)\n",
    "\n",
    "spectrogram_to_audio(spectrograms[2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаём нейронную сеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23d28fa8450>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_x, train_data_y = np.array(padded_texts), np.array(spectrograms)\n",
    "output_shape = spectrograms[0].shape\n",
    "model = Sequential()\n",
    "model.add( Dense(30, input_dim=len(padded_texts[0])) )\n",
    "for i in range(4):\n",
    "        model.add( Dense(units=20, activation='LeakyReLU') )\n",
    "model.add( Dense( output_shape[0]*output_shape[1], activation='LeakyReLU' ) )\n",
    "model.add( Reshape(output_shape)  )\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "model.fit(train_data_x, train_data_y\n",
    "                        , epochs=200\n",
    "                        , batch_size=64\n",
    "                        , verbose=0\n",
    "                        ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(151, 151)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(padded_texts),  len(spectrograms)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обрабатываем текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = ['привет как дела?']\n",
    "\n",
    "def text_to_pedded_text(texts):\n",
    "    # Text normalization and tokenization\n",
    "    normalized_texts = []\n",
    "    tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "\n",
    "    for text in texts:\n",
    "        # Normalize text by converting to lowercase and removing punctuation\n",
    "        normalized_text = text.lower()\n",
    "        # Tokenize the normalized text\n",
    "        tokens = tokenizer.texts_to_sequences([normalized_text])[0]\n",
    "        normalized_texts.append(tokens)\n",
    "\n",
    "    # Text encoding\n",
    "    encoded_texts = normalized_texts\n",
    "\n",
    "    # Padding\n",
    "    max_seq_length = max(len(seq) for seq in encoded_texts)\n",
    "    padded_texts = pad_sequences(encoded_texts, maxlen=max_seq_length, padding='post')\n",
    "\n",
    "    # Convert to numpy array\n",
    "    padded_texts = np.array(padded_texts)\n",
    "\n",
    "    # Vocabulary size\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    return padded_texts\n",
    "\n",
    "texts = text_to_pedded_text(word)\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\MateBook\\Documents\\pyt\\.conda\\Lib\\site-packages\\keras\\engine\\training.py\", line 2169, in predict_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\MateBook\\Documents\\pyt\\.conda\\Lib\\site-packages\\keras\\engine\\training.py\", line 2155, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\MateBook\\Documents\\pyt\\.conda\\Lib\\site-packages\\keras\\engine\\training.py\", line 2143, in run_step  **\n        outputs = model.predict_step(data)\n    File \"c:\\Users\\MateBook\\Documents\\pyt\\.conda\\Lib\\site-packages\\keras\\engine\\training.py\", line 2111, in predict_step\n        return self(x, training=False)\n    File \"c:\\Users\\MateBook\\Documents\\pyt\\.conda\\Lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\MateBook\\Documents\\pyt\\.conda\\Lib\\site-packages\\keras\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 12), found shape=(None, 3)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[39m.\u001b[39;49mpredict(texts)\n",
      "File \u001b[1;32mc:\\Users\\MateBook\\Documents\\pyt\\.conda\\Lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileys2yq8td.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\MateBook\\Documents\\pyt\\.conda\\Lib\\site-packages\\keras\\engine\\training.py\", line 2169, in predict_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\MateBook\\Documents\\pyt\\.conda\\Lib\\site-packages\\keras\\engine\\training.py\", line 2155, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\MateBook\\Documents\\pyt\\.conda\\Lib\\site-packages\\keras\\engine\\training.py\", line 2143, in run_step  **\n        outputs = model.predict_step(data)\n    File \"c:\\Users\\MateBook\\Documents\\pyt\\.conda\\Lib\\site-packages\\keras\\engine\\training.py\", line 2111, in predict_step\n        return self(x, training=False)\n    File \"c:\\Users\\MateBook\\Documents\\pyt\\.conda\\Lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\MateBook\\Documents\\pyt\\.conda\\Lib\\site-packages\\keras\\engine\\input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 12), found shape=(None, 3)\n"
     ]
    }
   ],
   "source": [
    "model.predict(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
